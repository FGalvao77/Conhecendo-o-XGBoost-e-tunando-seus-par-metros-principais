# Conhecendo o XGBoost e tunando seus parâmetros principais

XGBoost é uma biblioteca otimizada de aumento de gradiente distribuída projetada para ser altamente eficiente , flexível e portátil . Ele implementa algoritmos de aprendizado de máquina sob a estrutura [Gradient Boosting](https://en.wikipedia.org/wiki/Gradient_boosting). O XGBoost fornece um aumento de árvore paralela (também conhecido como GBDT, GBM) que resolve muitos problemas de ciência de dados de maneira rápida e precisa. O mesmo código é executado em grandes ambientes distribuídos (Hadoop, SGE, MPI) e pode resolver problemas além de bilhões de exemplos.

Vamos entender como funciona o `XGBoot` e, como melhor utilizá-lo `tunando` alguns parâmetros para obter o "melhor" modelo.

### **XGBoost**

- [Documentação](https://xgboost.readthedocs.io/en/stable/index.html)

- Material extra para consulta:
 - [Paper XGBoost](https://arxiv.org/pdf/1603.02754.pdf)
 - [Gradient Boosting na Wikipedia](https://en.wikipedia.org/wiki/Gradient_boosting)
 - [Paper Gradient Boosting](https://jerryfriedman.su.domains/ftp/trebst.pdf)

Segue o link do [notebook](https://colab.research.google.com/drive/11oU9lmlKskNoQW5VfBHrpR8MOYTRZT66#scrollTo=BaaTDu-YGEZq).
